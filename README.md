# РАЗРАБОТКА НЕЙРОИГРЫ С АДАПТИВНЫМ УПРАВЛЕНИЕМ ДЛЯ ТРЕНИНГА КОГНИТИВНЫХ СПОСОБНОСТЕЙ С ИСПОЛЬЗОВАНИЕМ ОБУЧЕНИЯ С ПОДКРЕПЛЕНИЕМ 

Целью работы является разработка нейроигры с адаптивным управлением для тренинга когнитивных способностей с использованием обучения с подкреплением. 

Задачи:

- определить целевую когнитивную функцию и сформулировать критерии оценки;
- разработать базовый прототип игры, ориентированной на тренировку выбранной когнитивной функции;
- формализовать задачу обучения с подкреплением: подобрать среду, пространства действий, награды и наблюдения;
- реализовать игровую среду со сбором данных о действиях пользователя и результатах прохождения заданий;
- обучить RL-агента на основе данных игровых логов и внедрить обученного агента в игровую среду;
- провести тестирование игры, собрать данные о прохождении;
- выполнить сравнительный анализ поведения симулятора и респондентов, оценить качество адаптации и влияние RL-модуля;

Результаты работы включают прототип игры для тренировки выбранной когнитивной функции с механизмом адаптивного управления уровнем сложности, 
а также обоснование выбранного алгоритма для построения модуля адаптивного управления. В рамках исследования проведен эксперимент, 
направленный на анализ изменения показателей прохождения при использовании адаптивной сложности. 

## Концепция игры (Deep Space Ops)

Игрок - оператор глубококосмической станции. На станции постоянно возникают сигналы и сбои, которые требуют быстрых решений.
Задача игрока - удерживать стабильность станции, одновременно обрабатывая несколько потоков задач.

Ключевые особенности:
- базовый формат: управление станцией + micro-задачи
- dual-task элементы для устойчивости внимания и переключения
- адаптивная сложность через RL-модуль

Цель игрового дизайна - не "тест", а полноценная игра, в которой когнитивная нагрузка естественно встроена в сюжет.

## Целевые когнитивные функции

- устойчивое внимание (sustained/selective attention)
- когнитивная гибкость и переключение задач (task switching)
- рабочая память
- скорость обработки и точность

## Micro-задачи (3 типа)

1. Сравнение кодов (Selective Attention)
   - сравнить два коротких сигнала: совпадают или нет
   - параметры сложности: длина кода, процент "почти одинаковых" пар, лимит времени

2. Последовательность (Working Memory)
   - запомнить короткую последовательность и ответить на вопрос после задержки
   - параметры сложности: длина последовательности, задержка, тип вопроса

3. Переключение правил (Task Switching)
   - менять правило сортировки стимулов (например, цвет/форма)
   - параметры сложности: частота смены правил, скорость подачи стимулов, сложность правил

## Сессия

Сессия определяется по количеству задач (а не по времени). Это дает стабильную базу для сравнения результатов и обучения агента.

## Формализация RL-задачи

### Среда
Игровая станция с потоками задач и micro-задачами.

### Наблюдения (state)
Состояние отражает текущую нагрузку и динамику игрока:
- последние метрики точности и времени реакции
- error_rate и switch_cost
- тренд усталости (fatigue_index)
- текущая нагрузка (parallel_streams)
- текущие параметры сложности

### Действия (action)
Агент управляет параметрами сложности:
- темп появления событий (event_rate)
- количество параллельных потоков (parallel_streams)
- общий коэффициент ограничения времени (time_pressure)
- сложность micro-задач (micro_diff_vector)

### Награда (reward)
Баланс между прогрессом игрока и сохранением оптимальной нагрузки:
- плюс за рост точности и скорости
- минус за резкий рост ошибок или перегрузку
- минус за слишком низкую сложность (скука, отсутствие развития)

### Эпизод
Одна игровая сессия (фиксированное число задач).

## Логирование

### События
- timestamp
- task_id, task_type
- difficulty_params
- expected_action
- user_action
- correct
- reaction_time
- deadline_met

### Сессии
- session_id, user_id (анонимизированный)
- total_tasks, accuracy_total
- mean_rt, rt_variance
- switch_cost
- fatigue_trend

### Для RL
- state_vector
- action_vector
- reward
- next_state_vector

## Метрики оценки

- accuracy_total
- mean_rt и rt_variance
- switch_cost (рост ошибок/RT после смены правил)
- устойчивость в нагрузке (fatigue_trend)
- удержание оптимальной зоны сложности

## Экспериментальная схема

Сравнение трех режимов:
1. Статическая сложность
2. Эвристическая адаптация
3. RL-адаптация

Критерии оценки: стабильность прогресса, снижение ошибок при росте сложности, удержание игрока в оптимальной зоне.

## Архитектура проекта (план)

- `game/` - игровой цикл, рендер, интерфейс
- `game/tasks/` - micro-задачи
- `adaptation/` - контроллер сложности и RL-агент
- `analytics/` - анализ логов и визуализация метрик
- `data/` - игровые логи и данные эксперимента
- `config/` - параметры экспериментов и настроек

## Дизайн и UX

- интерфейс оператора станции с панелями задач и индикаторами
- визуальная атмосфера Deep Space Ops
- понятные индикаторы ошибок и перегрузки
- минималистичный, но "живой" интерфейс с легкими анимациями

## Полная концепция и параметры адаптации

### Сюжет и роль игрока
Игрок выступает в роли оператора глубококосмической станции. Системы станции требуют постоянного контроля: связь, энергия, навигация,
безопасность. Игрок обрабатывает потоки сигналов и сбои, выполняя micro-задачи под ограничениями времени и нагрузки.

### Игровой цикл (сессия)
Сессия фиксируется по количеству задач (рекомендуемое значение - 120). В течение сессии чередуются и комбинируются три типа micro-задач.
Адаптивная система меняет темп и сложность, удерживая игрока в оптимальной зоне нагрузки.

### Базовые micro-задачи

1) Сравнение кодов (Selective Attention)
   - задача: определить, совпадают ли два коротких сигнала
   - когнитивные функции: селективное внимание, скорость обработки

2) Последовательность (Working Memory)
   - задача: запомнить последовательность и ответить на вопрос после задержки
   - когнитивные функции: рабочая память, устойчивость внимания

3) Переключение правил (Task Switching)
   - задача: быстро адаптироваться к смене правила сортировки стимулов
   - когнитивные функции: когнитивная гибкость, переключение задач

### Параметры адаптации (управляются RL/эвристикой)

Ниже перечислены параметры, которые составляют вектор сложности. Агент управляет ими, изменяя нагрузку на игрока.

#### Глобальные параметры нагрузки
- `event_rate`
  - описание: средний темп появления новых событий
  - влияние: повышает/снижает общую скорость игры и давление времени
  - рекомендуемый диапазон: 0.6-1.8 сек между событиями

- `parallel_streams`
  - описание: количество одновременно активных потоков задач
  - влияние: увеличивает требования к переключению внимания и многозадачности
  - рекомендуемый диапазон: 1-4

- `time_pressure`
  - описание: общий коэффициент ограничения времени (множитель для дедлайнов)
  - влияние: сокращает допустимое время реакции
  - рекомендуемый диапазон: 0.8-1.2

- `task_mix`
  - описание: распределение типов micro-задач внутри сессии
  - влияние: балансирует нагрузку по когнитивным функциям
  - рекомендуемые значения: гибкое распределение, например 40/30/30

#### Параметры micro-задачи 1: Сравнение кодов
- `code_len`
  - описание: длина кода/сигнала
  - влияние: увеличивает визуальную сложность
  - рекомендуемый диапазон: 3-7

- `similarity_rate`
  - описание: доля пар с минимальными отличиями
  - влияние: повышает вероятность ошибок при невнимательности
  - рекомендуемый диапазон: 0.2-0.7

- `time_limit`
  - описание: лимит времени на ответ
  - влияние: усиливает давление времени
  - рекомендуемый диапазон: 1600-700 мс

#### Параметры micro-задачи 2: Последовательность
- `seq_len`
  - описание: длина последовательности
  - влияние: увеличивает нагрузку на рабочую память
  - рекомендуемый диапазон: 3-8

- `retention_delay`
  - описание: задержка между показом и вопросом
  - влияние: увеличивает нагрузку на удержание информации
  - рекомендуемый диапазон: 400-1200 мс

- `question_type`
  - описание: тип вопроса (наличие символа / позиция / порядок)
  - влияние: меняет когнитивную стратегию
  - рекомендуемый режим для MVP: только "наличие символа"

#### Параметры micro-задачи 3: Переключение правил
- `rule_switch_rate`
  - описание: частота смены правила
  - влияние: повышает требования к когнитивной гибкости
  - рекомендуемый диапазон: 1 смена на 4-10 задач

- `stimulus_rate`
  - описание: скорость подачи стимулов
  - влияние: повышает требования к скорости реакции
  - рекомендуемый диапазон: 1.2-2.8 сек

- `rule_complexity`
  - описание: количество правил (2 или 3)
  - влияние: добавляет нагрузку на переключение и удержание правил
  - рекомендуемый режим для MVP: 2 правила

### Логика адаптации

Агент (RL или эвристика) наблюдает результаты игрока и меняет параметры сложности.
Базовый принцип: поддерживать состояние оптимальной нагрузки, избегая как скуки, так и перегрузки.

### Эвристический baseline (для сравнения с RL)
- окно оценки: последние 20 задач
- если accuracy > 85% и mean_rt < 900 мс => сложность +1
- если accuracy < 65% или mean_rt > 1400 мс => сложность -1
- проверка каждые 10 задач

### Метрики эффективности адаптации

- устойчивость точности (accuracy_total)
- стабильность времени реакции (mean_rt, rt_variance)
- стоимость переключения (switch_cost)
- устойчивость под нагрузкой (fatigue_trend)
- удержание в оптимальной зоне сложности

## Подготовка адаптации и обучения модели

### 1) Сбор данных в игре
- Запусти игру в baseline и/или adaptive режиме.
- Логи пишутся в:
  - `data/events.jsonl`
  - `data/adaptations.jsonl`
  - `data/sessions.jsonl`
- В `data/adaptations.jsonl` каждая запись содержит `session_id`, `batch_index`, `state`, `action_id`, `reward`, `mode`.
  - `action_space` = `tempo3` (действие кодирует только изменение темпа: `0=-1`, `1=0`, `2=+1`).
  - адаптация применяется по завершению батча (этапа), синхронно с логикой уровней.

### 2) Обучение модели
```bash
python -m training.train --data data/adaptations.jsonl --out data/ppo_agent.pt --epochs 40 --batch-size 64 --gamma 0.97 --mode all
```
Опции `--mode`:
- `all` — использовать все записи
- `baseline` — только baseline-сегменты
- `ppo` — только adaptive/ppo-сегменты

После обучения сохраняются:
- веса: `data/ppo_agent.pt`
- метаданные: `data/ppo_agent.meta.json`

### 3) Использование модели в игре
- Файл модели должен быть доступен по пути `data/ppo_agent.pt`.
- На стартовом экране переключи режим на `адаптивный`.

## Сборка Desktop-версии (PyInstaller)

Можно собрать исполняемый файл и архив для раздачи.

### Вариант 1: кнопкой Run в PyCharm
- Запусти файл: `packaging/build_desktop.py`

### Вариант 2: из терминала
```bash
python packaging/build_desktop.py --install-deps
```

После сборки:
- исполняемый файл: `dist/NeuroGame` (или `dist/NeuroGame.exe`)
- архив для распространения: `dist/neurogame-<platform>.zip`

Локальные данные приложения (логи/очереди) пишутся в системную папку пользователя:
- macOS: `~/Library/Application Support/NeuroGame/`
- Windows: `%APPDATA%\NeuroGame\`

В стартовом меню есть блок `Телеметрия`, где можно:
- увидеть текущий адрес отправки,
- проверить связь кнопкой `Проверить`,
- сохранить адрес кнопкой `Сохранить`.
Игра не блокируется при отсутствии связи: события копятся в локальной очереди и отправляются позже.

## Сборка в GitHub (Mac + Windows)

В проект добавлен workflow: `.github/workflows/build-release.yml`.

Как запускать:
1. Запушь изменения в GitHub.
2. Открой вкладку `Actions`.
3. Выбери workflow `Build Installers`.
4. Нажми `Run workflow`.
5. После завершения скачай артефакты:
- `neurogame-macos` (внутри `NeuroGame.dmg`)
- `neurogame-windows` (внутри `NeuroGameSetup.exe`)
